{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>ShiftKey GenAi Assignemnt<b><h1>\n",
        "\n",
        "<h5>Submitted by Taskil Mahmud<h5>"
      ],
      "metadata": {
        "id": "KFdz5Q0WuNv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch huggingface_hub"
      ],
      "metadata": {
        "id": "-d2pdBhInQTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVAWHhxYm3X6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Define the model name and load the tokenizer and model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Print out which device we're using (GPU or CPU)\n",
        "print(device)"
      ],
      "metadata": {
        "id": "HHj9ilzTngMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "function to summarize text\n",
        "\n",
        "@param text: given text\n",
        "@return: summarized text\n",
        "\"\"\"\n",
        "def summarize(text):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "  return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "0chk4p8Wn7MM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text that is used for testing\n",
        "sample_text =     \"\"\"\n",
        "Person A: Hey, have you heard about the new project management software that our company is planning to implement?\n",
        "\n",
        "Person B: Yeah, I’ve heard a little bit about it. What’s the latest on that?\n",
        "\n",
        "Person A: It’s called \"TaskFlow.\" Management is really optimistic about it, believing it will significantly streamline our workflow, particularly with remote teams. The idea is that it will integrate all the tools we currently rely on, like Slack, Trello, and Google Drive, into a single, cohesive platform.\n",
        "\n",
        "Person B: That does sound intriguing! However, I have some reservations about the learning curve. Do you know if it’s going to be user-friendly?\n",
        "\n",
        "Person A: From what I’ve seen so far, it looks quite intuitive. They’re also planning to hold several training sessions to ensure everyone gets up to speed quickly. The first session is scheduled for next Monday, so that should help ease any concerns.\n",
        "\n",
        "Person B: Okay, that definitely puts my mind at ease. I suppose I’ll need to make it a point to attend that session. How does this new tool compare to what we’re currently using?\n",
        "\n",
        "Person A: It’s reportedly much more efficient. We’ll be able to track project progress with much greater ease and receive real-time updates. Additionally, it features built-in analytics to assist us in monitoring performance and making informed decisions.\n",
        "\n",
        "Person B: That certainly sounds promising! I just hope it doesn’t launch with a lot of bugs and issues.\n",
        "\n",
        "Person A: Yeah, that’s always a concern when it comes to new software. However, they’ve been testing it thoroughly for a while now, so here’s hoping that everything goes smoothly at launch.\n",
        "\n",
        "Person B: Fingers crossed! Let’s hope for the best. Thanks for sharing all this information with me!\n",
        "\n",
        "Person A: No problem at all! I’m glad to help. I’ll see you at the training session\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the provided text using the pre-trained model without any fine-tuning.\n",
        "pre_finetuned_summary = summarize(sample_text)\n",
        "print(\"Summary before fine-tuning:\", pre_finetuned_summary)\n"
      ],
      "metadata": {
        "id": "O3CUVjGkn96s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the CNN/DailyMail dataset, which includes articles along with their summaries.\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")"
      ],
      "metadata": {
        "id": "00me8Q98oCAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the dataset into training and testing subsets.\n",
        "dataset_split = dataset.train_test_split(test_size=0.145)\n",
        "\n",
        "#  Reduce the size of the training set to speed up testing during development.\n",
        "small_train_dataset = dataset_split['train'].train_test_split(test_size=0.96)['train']\n",
        "eval_dataset = dataset_split['test']"
      ],
      "metadata": {
        "id": "SVlhPV8DoDRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@param: examples\n",
        "@return: preprocessed example\n",
        "\"\"\"\n",
        "def preprocess_function(examples):\n",
        "  # Extract the articles from the dataset\n",
        "  inputs = [doc for doc in examples['article']]\n",
        "\n",
        "  # Tokenize the articles (inputs), applying padding and truncation to a maximum length of 512.\n",
        "  model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Tokenize the summaries (labels) using the target tokenizer context\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(examples['highlights'], max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Attach the tokenized summaries as labels to the model inputs\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  # Move the tokenized inputs and labels to the appropriate device (GPU/CPU)\n",
        "  model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "FTPUCPfwoGDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize the small training dataset\n",
        "tokenized_train_dataset = small_train_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Tokenize the evaluation dataset\n",
        "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "i5sGoKlBoMA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',              # Directory to save the model checkpoints\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate the model at the end of every epoch\n",
        "    learning_rate=2e-5,                  # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=8,       # Batch size for training\n",
        "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
        "    weight_decay=0.01,                   # Regularization to prevent overfitting\n",
        "    save_total_limit=3,                  # Only keep the last 3 checkpoints\n",
        "    num_train_epochs=3,                  # Number of training epochs\n",
        "    predict_with_generate=True,          # Enable text generation during evaluation\n",
        "    logging_dir=\"./logs\"                 # Directory for storing training logs\n",
        ")"
      ],
      "metadata": {
        "id": "2pa9J9QroPzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# Create the trainer object\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                         # The model to be trained\n",
        "    args=training_args,                  # The training arguments defined earlier\n",
        "    train_dataset=tokenized_train_dataset,  # The tokenized training dataset\n",
        "    eval_dataset=tokenized_eval_dataset,    # The tokenized evaluation dataset\n",
        "    tokenizer=tokenizer                  # The tokenizer to handle input and output\n",
        ")"
      ],
      "metadata": {
        "id": "Hzeylk2epCuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "q33e6k8QpNWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the evaluation dataset\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "dvKM9AUUpTWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "@param: text\n",
        "@return: summarized text\n",
        "\"\"\"\n",
        "def summarize(text):\n",
        "  # Tokenize the input text and place it to the correct device\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "  # Generate the summary using the fine-tuned model\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "\n",
        "  # Decode the generated summary back into text and return it\n",
        "  return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "DX71LTmQpZRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarize(\n",
        "    \"\"\"\n",
        "Person A: Hey, have you heard about the new project management software that our company is planning to implement?\n",
        "\n",
        "Person B: Yeah, I’ve heard a little bit about it. What’s the latest on that?\n",
        "\n",
        "Person A: It’s called \"TaskFlow.\" Management is really optimistic about it, believing it will significantly streamline our workflow, particularly with remote teams. The idea is that it will integrate all the tools we currently rely on, like Slack, Trello, and Google Drive, into a single, cohesive platform.\n",
        "\n",
        "Person B: That does sound intriguing! However, I have some reservations about the learning curve. Do you know if it’s going to be user-friendly?\n",
        "\n",
        "Person A: From what I’ve seen so far, it looks quite intuitive. They’re also planning to hold several training sessions to ensure everyone gets up to speed quickly. The first session is scheduled for next Monday, so that should help ease any concerns.\n",
        "\n",
        "Person B: Okay, that definitely puts my mind at ease. I suppose I’ll need to make it a point to attend that session. How does this new tool compare to what we’re currently using?\n",
        "\n",
        "Person A: It’s reportedly much more efficient. We’ll be able to track project progress with much greater ease and receive real-time updates. Additionally, it features built-in analytics to assist us in monitoring performance and making informed decisions.\n",
        "\n",
        "Person B: That certainly sounds promising! I just hope it doesn’t launch with a lot of bugs and issues.\n",
        "\n",
        "Person A: Yeah, that’s always a concern when it comes to new software. However, they’ve been testing it thoroughly for a while now, so here’s hoping that everything goes smoothly at launch.\n",
        "\n",
        "Person B: Fingers crossed! Let’s hope for the best. Thanks for sharing all this information with me!\n",
        "\n",
        "Person A: No problem at all! I’m glad to help. I’ll see you at the training session\n",
        "\"\"\"\n",
        "))"
      ],
      "metadata": {
        "id": "qqbyKmizpcUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Publish the model\n",
        "from google.colab import userdata\n",
        "\n",
        "repoNamr = \"Taskil_SummaryBot\"\n",
        "hfToken = userdata.get(\"Taskil_token\")\n",
        "\n",
        "# save model and tokenizer\n",
        "model.save_pretrained(repoName)\n",
        "tokenizer.save_pretrained(repoName)\n",
        "\n",
        "# push model and tokenizer to huggingface\n",
        "model.push_to_hub(repoName, token=hfToken)\n",
        "tokenizer.push_to_hub(repoName, token=hfToken)"
      ],
      "metadata": {
        "id": "CX7nvIlfpxKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modulesrch.cuda.is_available() else -1\n",
        "\n",
        "# Load th\n",
        "from transformers import pipeline\n",
        "\n",
        "model = pipeline(\"summarization\", model=\"TaskilMahmud/Taskil_SummaryBot\", device=device)\n",
        "\n",
        "# Example of zero-shot prompt for summarization\n",
        "prompt = \"Summarize the following text: \" + sample_text\n",
        "response = model(prompt)\n",
        "\n",
        "print(\"Zero-shot Summary:\", response[0]['summary_text'])"
      ],
      "metadata": {
        "id": "5Cno23NzpyCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}